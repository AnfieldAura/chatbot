{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b9974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the cleaned JSON data\n",
    "with open(\"ccleaned_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7ddddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Load the cleaned JSON file\n",
    "with open(\"ccleaned_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Search function using fuzzy matching\n",
    "def search_query(query, data, n_results=3):\n",
    "    questions = [item[\"text\"] for item in data]\n",
    "    matches = get_close_matches(query, questions, n=n_results, cutoff=0.3)\n",
    "    \n",
    "    results = []\n",
    "    for match in matches:\n",
    "        for item in data:\n",
    "            if item[\"question\"] == match:\n",
    "                results.append({\"question\": item[\"question\"], \"answer\": item[\"answer\"]})\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = \"how do I reset my password?\"\n",
    "results = search_query(query, data)\n",
    "\n",
    "# Display results\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(\"Q:\", res[\"question\"])\n",
    "    print(\"A:\", res[\"answer\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be797b9-9196-4427-af8f-b533ac39e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f04c749-7389-4d05-8761-4ea98ed57a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAISS Index Created & Saved as 'index.faiss'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Convert text into an embedding\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Generate Embeddings for Each Chunk\n",
    "text_chunks = [entry[\"text\"] for entry in data]\n",
    "embeddings = np.array([get_embedding(text)[0] for text in text_chunks], dtype=\"float32\")\n",
    "\n",
    "# Create FAISS Index\n",
    "embedding_size = embeddings.shape[1]  # Get embedding size dynamically\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS Index\n",
    "faiss_index_file = \"index.faiss\"\n",
    "faiss.write_index(index, faiss_index_file)\n",
    "\n",
    "print(\" FAISS Index Created & Saved as 'index.faiss'.\")\n",
    "\n",
    "### Step 3: Search in FAISS ###\n",
    "\n",
    "# Load FAISS Index\n",
    "if not os.path.exists(faiss_index_file):\n",
    "    print(f\" Error: '{faiss_index_file}' not found. Please ensure the index was created properly.\")\n",
    "    exit()\n",
    "\n",
    "index = faiss.read_index(faiss_index_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7963f52f-fbcc-4f5d-a84b-afaf13e959ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Saved as 'cleaned_data.json'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = \"scrape11.txt\"\n",
    "output_json = \"cleaned_data.json\"\n",
    "faiss_index_file = \"index.faiss\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: '{input_file}' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Clean the Text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize spaces\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text  # Preserve case & punctuation for better meaning\n",
    "\n",
    "\n",
    "# Apply cleaning and remove duplicates\n",
    "cleaned_data = list(set([clean_text(line) for line in data if line.strip() != \"\"]))\n",
    "\n",
    "# Chunking (Split Large Text into Small Sections)\n",
    "def chunk_text(text_list, chunk_size=200):\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for line in text_list:\n",
    "        if len(chunk) + len(line) <= chunk_size:\n",
    "            chunk += \" \" + line\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = line\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())  # Add last chunk if needed\n",
    "    return chunks\n",
    "\n",
    "final_chunks = chunk_text(cleaned_data, chunk_size=200)\n",
    "\n",
    "# Save Processed Data as JSON\n",
    "json_data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(final_chunks)]\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(json_data, file, indent=4)\n",
    "\n",
    "print(\"Preprocessed Data Saved as 'cleaned_data.json'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42579e32-e42f-45ba-aa89-36b780e73c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top Matches:\n",
      "\n",
      " Match 1: Director Academic - Mrs. Deepa Ganu: Mrs. Deepa Ganu is a proactive and student-friendly academic director at KMIT. With a background in B.Tech (ECE) from Pune University and M.Tech from JNTU Hyderabad, she has been instrumental in mentoring students for various national and international platforms like Microsoft Imagine Cup, IBM's Great Mind Challenge, Google Summer of Code, and Amazon Campus Mentorship Series. She has also conducted corporate training for reputed companies like Verizon, HP, and CA Global, as well as government agencies such as DRDL and RCI. Mrs. Ganu is the author of the book 'Java Spoken Tutorials' under IIT Bombay's initiative and has received prestigious titles like 'Best Mentor' and 'Acharya' from major corporate companies. She has also been honored with the Drona Award for her contribution to pedagogy.\n",
      "\n",
      " Match 2: About the Humanities & Science (HS) Department: Established in 2007, the department offers a current intake of 840 students, with 540 in CSE and 300 in CSM. The department focuses on laying strong foundations in basic sciences and language skills, which are essential for the advanced learning of technology students. The department's mission includes training students in the latest software technologies, fostering a passion for algorithmic thought processes, and encouraging lifelong learning through multi-modal platforms. The program aims to create socially conscious, well-rounded individuals who contribute to India's technological and entrepreneurial growth. Program outcomes include proficiency in engineering knowledge, problem-solving, design, investigation, modern tool usage, ethics, team work, communication, project management, and lifelong learning.\n",
      "\n",
      " Match 3: Faculty in HS Department: The department has a diverse set of faculty members with expertise in various disciplines such as mathematics, physics, chemistry, English, and mechanical engineering. Notable faculty include: 1. Mr. Balakrishna Ushakoyala, HOD, MSc PhD (Mathematics). 2. Mrs. Saritha Gone, HOD, MBA PhD (Finance). 3. Dr. Padmanabha Sastry, Associate Professor, PhD (Mathematics). 4. Dr. Srinivas Mangipudi, Associate Professor, PhD (Physics). 5. Dr. Siperambudur Udaya Laxmi, Associate Professor, PhD (Physics). 6. Dr. Bhikshu N L, Associate Professor, PhD (Mathematics). 7. Dr. Devika Rubi, Associate Professor, PhD (Mathematics). 8. Dr. Pramada Jayanthi, Associate Professor, PhD (Mathematics). 9. Dr. Seshu Bhamidipati, Associate Professor, PhD (Physics). 10. Dr. K Swapna, Associate Professor, PhD (Chemistry).\n",
      "\n",
      " Match 4: Contact Us: Timings are from 9:30 AM to 4:00 PM on working days. Faculty contacts: 1. Mr. Balakrishna Ushakoyala, Assistant Professor, HOD, HS (Email: hshod1@kmit.in). 2. Mrs. Saritha Gone, Assistant Professor, HOD, HS (Email: hshod2@kmit.in).\n",
      "\n",
      " Match 5: Heads of Departments: 1. Dr. Vemula Aruna: CSE HOD. 2. Ms. Rupa Devi: Incharge HOD. 3. Ms. Priyanka Saxena: Incharge HOD. 4. Mr. Para Upendar: Incharge HOD. 5. Dr. G. Narender: IT HOD. 6. Dr. T.V.G. Sridevi: CSE-AI/ML HOD. 7. Mr. Kanil: CSE-Data Science HOD. 8. Ms. Saritha Gone: HS HOD. 9. Dr. Balakrishna Ushakoyala: HS HOD.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text=\"Director Academic\"\n",
    "query_embedding = get_embedding(query_text).astype(\"float32\")\n",
    "\n",
    "k = 5  # Retrieve more results for better accuracy\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "retrieved_docs = [text_chunks[i] for i in indices[0]]\n",
    "\n",
    "print(\"\\n Top Matches:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\" Match {i}: {doc}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
