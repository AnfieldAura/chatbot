{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7963f52f-fbcc-4f5d-a84b-afaf13e959ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Step 1 Completed: Preprocessed Data Saved as 'cleaned_data.json'.\n",
      "‚úÖ Step 2 Completed: FAISS Index Created & Saved as 'index.faiss'.\n",
      "\n",
      "üîç Top Matches: ['btech programs the eligibility criteria for admission to btech program admissions to the btech program are made along with the other engineering colleges in the state through a common entrance test eapcet conducted by the govt of telangana state the admission pattern to b tech is as follows the minimum qualification for admission to first year of the b tech course is a pass in the intermediate 10 2 conducted by the board of intermediate education govt of telangana state or any other examination recognized as equivalent thereto with mathematics physics and chemistry as optional subjects admission 70 of the seats are allotted based on the merit in the eapcet 30 of the seats are earmarked for managementnri candidates in addition to the above diploma holders are admitted in second year of b tech to the extent of 20 of intake based on the merit in the ecet under lateral entry scheme fee structure year of admission 202425 btechfour year duration 202425 i year 202526 ii year 202627 iii year 202728 iv year tuition fee 103000 103000 103000 103000 special fee 5500 2500 2500 2500 nba fee 3000 3000 3000 3000 for more information please contact the admissions department at 6302140205 admission procedure to apply online click here', 'consultancy projects crimefreebharat an mp police initiative other consultancy projects crime free bharat mission is dedicated to creating holistic and comprehensive solutions for creating crime free bharat focus is on internet protocols cybercrimes cyber forensic and investigation and cyber security kmit and ngit signed an mou to work together on the above initiative this required both faculty members and students of both the institutes to work with with shri maithili sharan gupta dgp special reforms mp gov to create solutions that would align with the goals of crime free bharat initiative start date june 25th 2020 end date sep 25th 2020 projects below are the projects that were part of the internship program sno title description 1 epayment service epayment service using bank accounts debit cards credit cards and all other options 2 smart investigation app smart investigation app and investigation support module for community app 3 evidence requisition system requisitioning of information from various sources such a telecom service providers b banks and financial institutions c reservation systems trains bus and aeroplanes d cctv systems and iot devices e revenue cooperative society and others 4 bharat crypto sign micro service back end service capturing signatures a photo signature b optical signature c digital signature otp verified d virtual signature with real time encryption with date time and location stamp and protection through block chain technology this service would be integrated wherever required 5 doshi anveshan yantr investigation related database management data mining and data analytics creating database as attributes of criminal profiles a facial features facial expressions gait bmi apparel vehicles and associates with date time and location stamp frequency of use association with crime related happening b crime related information criminal profiles c route related information d criminal cluster and hot spots e triggering task commands to cctv system and iot devices f identifying evidence and packaging them with certificate of integrity certificate of admissibility and certificate of chain of command students below are the students who were part of the program sno name college 1 mr a sriram reddy kmit 2 mr vivek rupender gopu kmit 3 mr kathuroju harsha vardhan kmit 4 mr telukunta vijay abhinav kmit 5 ms sweta vooda kmit 6 mr p sai varshith kmit 7 mr manda hrithik kmit 8 mr vegi shanmukh chowdary kmit 9 mr vaasrith kmit 10 mr krishna deep yerramallu kmit 11 mr abhishek dhatrika kmit 12 mr sai venkat arjun pathy kmit 13 mr trishool parvataneni kmit 14 mr ritish murki kmit 15 mr mithilesh kumar devaki kmit 16 mr pranay kmit 17 mr shubham jain kmit 18 ms namira kmit 19 mr murki sai srikanth kmit 20 mr syed zeeshan abbas jafferi ngit 21 mr ayush modi ngit 22 mr jammula dinesh reddy ngit 23 mr bheemreddy soumya reddy ngit', 'academic core committee key functions of academic core committee subjects are allocated based on the area of expertise and their teaching experience the faculty members are encouraged to attend refresher courses gap analysis is performed and appropriate measures are taken strategy to achieve maximum outcome for the course is devised course delivery is continuously monitored periodic feedback is considered for taking corrective measures student academic reviews are monitored the academic core committee then follows the process used to identify extent of compliance of the university curriculum for attaining the pos and psos the process is broadly divided into 2 categories as mentioned below and also attributes of nba like pos psos peos cos etc are considered direct assessment indirect assessment members of the academic core committee for the year 202324 are as follows sno name of the member department designation 1 dr bl malleswari principal kmit chairperson 2 dr k bhargavi cse aiml convenor 3 dr s udaya lakshmi coe member 4 mr g rakesh reddy cse member 5 mr a l narsimha rao hs member 6 ms m saradamani hs member 7 dr m anuradha cse member 8 dr vishal reddy cse ds member']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "### Step 1: Load and Preprocess Data ###\n",
    "\n",
    "# Load the Scraped Data\n",
    "input_file = \"scrape11.txt\"\n",
    "output_json = \"cleaned_data.json\"\n",
    "faiss_index_file = \"index.faiss\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"‚ùå Error: '{input_file}' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Clean the Text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces and newlines\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    return text.strip().lower()  # Convert to lowercase\n",
    "\n",
    "# Apply cleaning and remove duplicates\n",
    "cleaned_data = list(set([clean_text(line) for line in data if line.strip() != \"\"]))\n",
    "\n",
    "# Chunking (Split Large Text into Small Sections)\n",
    "def chunk_text(text_list, chunk_size=200):\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for line in text_list:\n",
    "        if len(chunk) + len(line) <= chunk_size:\n",
    "            chunk += \" \" + line\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = line\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())  # Add last chunk if needed\n",
    "    return chunks\n",
    "\n",
    "final_chunks = chunk_text(cleaned_data, chunk_size=200)\n",
    "\n",
    "# Save Processed Data as JSON\n",
    "json_data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(final_chunks)]\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(json_data, file, indent=4)\n",
    "\n",
    "print(\"‚úÖ Step 1 Completed: Preprocessed Data Saved as 'cleaned_data.json'.\")\n",
    "\n",
    "### Step 2: Generate Embeddings and Create FAISS Index ###\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Convert text into an embedding\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Generate Embeddings for Each Chunk\n",
    "text_chunks = [entry[\"text\"] for entry in json_data]\n",
    "embeddings = np.array([get_embedding(text)[0] for text in text_chunks], dtype=\"float32\")\n",
    "\n",
    "# Create FAISS Index\n",
    "embedding_size = embeddings.shape[1]  # Get embedding size dynamically\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS Index\n",
    "faiss.write_index(index, faiss_index_file)\n",
    "print(\"‚úÖ Step 2 Completed: FAISS Index Created & Saved as 'index.faiss'.\")\n",
    "\n",
    "### Step 3: Search in FAISS ###\n",
    "\n",
    "# Load FAISS Index\n",
    "if not os.path.exists(faiss_index_file):\n",
    "    print(f\"‚ùå Error: '{faiss_index_file}' not found. Please ensure the index was created properly.\")\n",
    "    exit()\n",
    "\n",
    "index = faiss.read_index(faiss_index_file)\n",
    "\n",
    "# Example Query\n",
    "query_text = \"What is the process for applying?\"\n",
    "query_embedding = get_embedding(query_text).astype('float32')\n",
    "\n",
    "# Search in FAISS\n",
    "k = 3  # Retrieve top 3 results\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Retrieve the actual text documents\n",
    "retrieved_docs = [text_chunks[i] for i in indices[0]]\n",
    "print(\"\\nüîç Top Matches:\", retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b890b8b3-2dbe-4eb5-9919-62dfc95d3099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.56.49:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import faiss\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Allows frontend (React) to call the API\n",
    "\n",
    "# Load Preprocessed Data\n",
    "with open(\"cleaned_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Load FAISS Index\n",
    "faiss_index_file = \"index.faiss\"\n",
    "index = faiss.read_index(faiss_index_file)\n",
    "\n",
    "# Load Model & Tokenizer for Embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def get_embedding(text):\n",
    "    \"\"\"Converts text into an embedding\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "@app.route(\"/search\", methods=[\"POST\"])\n",
    "def search():\n",
    "    try:\n",
    "        # Get user query\n",
    "        user_query = request.json.get(\"query\", \"\")\n",
    "        if not user_query:\n",
    "            return jsonify({\"error\": \"Query cannot be empty\"}), 400\n",
    "\n",
    "        # Convert query to embedding\n",
    "        query_embedding = get_embedding(user_query).astype('float32')\n",
    "\n",
    "        # Search FAISS Index\n",
    "        k = 3  # Number of top results\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "        # Retrieve matched text chunks\n",
    "        retrieved_docs = [data[i][\"text\"] for i in indices[0]]\n",
    "\n",
    "        return jsonify({\"query\": user_query, \"results\": retrieved_docs})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be797b9-9196-4427-af8f-b533ac39e1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
