{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9be797b9-9196-4427-af8f-b533ac39e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7963f52f-fbcc-4f5d-a84b-afaf13e959ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Saved as 'cleaned_data.json'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = \"scrape11.txt\"\n",
    "output_json = \"cleaned_data.json\"\n",
    "faiss_index_file = \"index.faiss\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: '{input_file}' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Clean the Text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize spaces\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text  # Preserve case & punctuation for better meaning\n",
    "\n",
    "\n",
    "# Apply cleaning and remove duplicates\n",
    "cleaned_data = list(set([clean_text(line) for line in data if line.strip() != \"\"]))\n",
    "\n",
    "# Chunking (Split Large Text into Small Sections)\n",
    "def chunk_text(text_list, chunk_size=200):\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for line in text_list:\n",
    "        if len(chunk) + len(line) <= chunk_size:\n",
    "            chunk += \" \" + line\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = line\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())  # Add last chunk if needed\n",
    "    return chunks\n",
    "\n",
    "final_chunks = chunk_text(cleaned_data, chunk_size=200)\n",
    "\n",
    "# Save Processed Data as JSON\n",
    "json_data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(final_chunks)]\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(json_data, file, indent=4)\n",
    "\n",
    "print(\"Preprocessed Data Saved as 'cleaned_data.json'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f04c749-7389-4d05-8761-4ea98ed57a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAISS Index Created & Saved as 'index.faiss'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Convert text into an embedding\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Generate Embeddings for Each Chunk\n",
    "text_chunks = [entry[\"text\"] for entry in json_data]\n",
    "embeddings = np.array([get_embedding(text)[0] for text in text_chunks], dtype=\"float32\")\n",
    "\n",
    "# Create FAISS Index\n",
    "embedding_size = embeddings.shape[1]  # Get embedding size dynamically\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS Index\n",
    "faiss.write_index(index, faiss_index_file)\n",
    "print(\" FAISS Index Created & Saved as 'index.faiss'.\")\n",
    "\n",
    "### Step 3: Search in FAISS ###\n",
    "\n",
    "# Load FAISS Index\n",
    "if not os.path.exists(faiss_index_file):\n",
    "    print(f\" Error: '{faiss_index_file}' not found. Please ensure the index was created properly.\")\n",
    "    exit()\n",
    "\n",
    "index = faiss.read_index(faiss_index_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "059c3e0a-2fa9-4a24-8f77-b935d49f81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " query_text = \"director academic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42579e32-e42f-45ba-aa89-36b780e73c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top Matches:\n",
      "\n",
      " Match 1: contact us timings from 0930am to 0400pm only on working days sno name of the faculty designation role email id 1 drtvg sridevi assistant professor hod aimlhodkmitin\n",
      "\n",
      " Match 2: director academic mrs deepa ganu director academic about deepa ganu deepa ganu the charismatic academic director of keshav memorial institute of technology has always been a proactive person and she has constructively involved herself in all the activities of the college she graduated b tech in ece from pune university and mtech from jntu hyderabad she consistently displays a fervent zeal in the academic progress of the students and the staff she strongly believes in herself and has immense trust in her mentors her peers and her team she took up a mission to ignite the young minds and nurture them with the potential to reach greater horizons she is a studentfriendly person and her forte is to identify their technical edge and channelize their skills in the right direction with her valuable guidance she trained and shared her expertise with thousands of students who could excel and compete on various national and international platforms like microsoft imagine cup ibms the great mind challenge google summer of code and amazon campus mentorship series she conducted various corporate training programmes for reputed campuses like verizon hp and ca global and for government agencies like drdl and rci she dares to dream works smart strives for excellence and enjoys every step of her journey all the way which made her scale greater heights she authored a book titled java spoken tutorials an iit bombay initiative big names in the corporate world like ibm google and virtusa bestowed her with the prestigious titles best mentor and acharya for her unprecedented contribution to pedagogy she has been an inspiration and role model in the field of education and that made her the recipient of the prestigious drona award message welcome to kmit the revered sanctuary of learning with a proven track record of ace placements and students achieving stellar results in academics we would like to invite you to explore the learning opportunities here and pursue a disciplined learning path that will lead to a beautiful career ahead the campus its instructors await with open arms to nourish your talents and witness your unstoppable success all of which will be written in golden letters in the history of kmit mrs deepa ganu\n",
      "\n",
      " Match 3: contact us timings from 0930am to 0400pm only on working days sno name of the faculty designation role email id 1 mrbalakrishna ushakoyala assistant professor hod hshod1kmitin 2 mrsaritha gone assistant professor hod hshod2kmitin\n",
      "\n",
      " Match 4: contact us timings from 0930am to 0400pm only on working days sno name of the faculty designation role email id 1 mrkanil assistant professor hod dshodkmitin\n",
      "\n",
      " Match 5: heads of the department dr vemula aruna csehod mstrupa devi inchargehod ms priyanka saxena inchargehod mr para upendar inchargehod dr g narender ithod drtvg sridevi cseaimlhod mrkanil csedata sciencehod mssaritha gone hshod dr balakrishna ushakoyala hshod\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_embedding = get_embedding(query_text).astype(\"float32\")\n",
    "\n",
    "k = 5  # Retrieve more results for better accuracy\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "retrieved_docs = [text_chunks[i] for i in indices[0]]\n",
    "\n",
    "print(\"\\n Top Matches:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\" Match {i}: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b42fe29-5462-415d-ab64-e4ab25da19c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
