{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxOKpFnmXTqO",
        "outputId": "5de56e61-19da-4051-de3f-71c82413c652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "H0S8VUQhXc9Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "You first install spaCy and download a small English language model (en_core_web_sm).\n",
        "\n",
        "import spacy brings the library into your script.\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\") loads the pre-trained model, which is required for all subsequent analysis.\n",
        "This setup step is necessary for any spaCy-based application."
      ],
      "metadata": {
        "id": "jieDpNaWX33J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "tj0HEr0vYUA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"spaCy is an advanced NLP library.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJHzpd3dX9Sk",
        "outputId": "75a4b9c9-f14c-410b-959e-66fd4306af47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy\n",
            "is\n",
            "an\n",
            "advanced\n",
            "NLP\n",
            "library\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Tokenization is splitting a text into basic units called tokens (words, punctuation, etc.).\n",
        "\n",
        "doc = nlp(text) processes the input string, producing a spaCy Doc object.\n",
        "\n",
        "Iterating over doc yields each token as an object; token.text gives the original text for each token.\n",
        "\n",
        "This step is foundational, enabling detailed analysis of text structure and meaning."
      ],
      "metadata": {
        "id": "CPGlCRAvYZyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Segmentation"
      ],
      "metadata": {
        "id": "YMNHfjMDYiJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing enables computers to understand human language. It's a fast-growing field!\"\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyC7x4tAYSM1",
        "outputId": "d5044cb3-8412-4f90-a848-feef58dd85a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing enables computers to understand human language.\n",
            "It's a fast-growing field!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Sentence segmentation divides text into sentences.\n",
        "\n",
        "Each sent in doc.sents is a sentence object.\n",
        "\n",
        "Accurate sentence splitting is necessary before more advanced tasks (e.g., parsing, information extraction)."
      ],
      "metadata": {
        "id": "aYCr-qfJYwFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "jvzdc6v9Y5Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"spaCy processes text efficiently.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(f\"{token.text} - {token.pos_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwN31P2kY0_Z",
        "outputId": "971351a0-f766-46be-bcee-9b4112600676"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy - X\n",
            "processes - NOUN\n",
            "text - NOUN\n",
            "efficiently - ADV\n",
            ". - PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "POS tagging labels each word with its grammatical category (noun, verb, etc.).\n",
        "\n",
        "token.pos_ provides the POS tag (e.g., 'NOUN', 'VERB').\n",
        "\n",
        "This process is crucial for understanding syntax and structure, supporting tasks like parsing and information extraction"
      ],
      "metadata": {
        "id": "k0StyDg0ZHs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "Uao21F_bZNkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The leaves are falling.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(f\"{token.text} - Lemma: {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpcURHDMZJ-0",
        "outputId": "4f0d0ec8-985c-45cf-a9e3-89468c7bfd6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The - Lemma: the\n",
            "leaves - Lemma: leave\n",
            "are - Lemma: be\n",
            "falling - Lemma: fall\n",
            ". - Lemma: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Lemmatization reduces each word to its dictionary (base) form.\n",
        "\n",
        "For example, \"leaves\" → \"leaf\", \"falling\" → \"fall\".\n",
        "\n",
        "This helps normalize vocabulary, supporting search and analysis tasks."
      ],
      "metadata": {
        "id": "Xjld3TejZglc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "VDeHTgssZmVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HslJFLSAZh_7",
        "outputId": "70d1db9f-aac3-499c-c618-d784a40a4451"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple, Label: ORG\n",
            "Entity: U.K., Label: GPE\n",
            "Entity: $1 billion, Label: MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "NER identifies real-world entities like names, places, and monetary amounts in text.\n",
        "\n",
        "Each detected entity (ent) has a label (e.g., 'ORG', 'GPE', 'MONEY').\n",
        "\n",
        "Recognizing entities enables applications like information extraction, search, and question answering."
      ],
      "metadata": {
        "id": "K8moTwajZ2R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Stop Word Detection"
      ],
      "metadata": {
        "id": "WSds3NSHZ662"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a simple test.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(f\"{token.text} - Stopword: {token.is_stop}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP9OdhUUZ3zk",
        "outputId": "3a4bafb8-9666-43a5-af2d-cb9a14cb287e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This - Stopword: True\n",
            "is - Stopword: True\n",
            "a - Stopword: True\n",
            "simple - Stopword: False\n",
            "test - Stopword: False\n",
            ". - Stopword: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Stop words are common words (like \"is\", \"a\", \"the\") that usually carry little semantic meaning.\n",
        "\n",
        "token.is_stop is True for stop words.\n",
        "\n",
        "Removing stop words simplifies text and focuses analysis on keywords."
      ],
      "metadata": {
        "id": "ywzIbwmFaCWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dependency Parsing"
      ],
      "metadata": {
        "id": "bBm5D0izaICl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"spaCy lets you analyze linguistic structure easily.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(f\"{token.text} <--{token.dep_}-- {token.head.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp1ZK6QBabUF",
        "outputId": "dde0b21f-37ef-4ee5-a669-90bee76463f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy <--meta-- lets\n",
            "lets <--ROOT-- lets\n",
            "you <--nsubj-- analyze\n",
            "analyze <--ccomp-- lets\n",
            "linguistic <--amod-- structure\n",
            "structure <--dobj-- analyze\n",
            "easily <--advmod-- analyze\n",
            ". <--punct-- lets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Dependency parsing analyzes sentence structure by describing how words depend on each other.\n",
        "\n",
        "token.dep_ is the type of grammatical relationship (subject, object, etc.).\n",
        "\n",
        "token.head.text is the word this token depends on.\n",
        "\n",
        "Understanding dependencies is key for extracting who does what to whom, supporting tasks like translation and question-answering"
      ],
      "metadata": {
        "id": "OSz8NtDWa70c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined Workflow Example"
      ],
      "metadata": {
        "id": "zYQyurS-bFpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Sentences\n",
        "for sent in doc.sents:\n",
        "    print(\"Sentence:\", sent.text)\n",
        "    for token in sent:\n",
        "        print(f\"  {token.text}: POS={token.pos_}, Lemma={token.lemma_}, Stopword={token.is_stop}\")\n",
        "    for ent in sent.ents:\n",
        "        print(f\"  Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AsMRDp9a_a3",
        "outputId": "2215c561-cc93-43ee-96a7-ccecb467da1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Barack Obama was born in Hawaii.\n",
            "  Barack: POS=PROPN, Lemma=Barack, Stopword=False\n",
            "  Obama: POS=PROPN, Lemma=Obama, Stopword=False\n",
            "  was: POS=AUX, Lemma=be, Stopword=True\n",
            "  born: POS=VERB, Lemma=bear, Stopword=False\n",
            "  in: POS=ADP, Lemma=in, Stopword=True\n",
            "  Hawaii: POS=PROPN, Lemma=Hawaii, Stopword=False\n",
            "  .: POS=PUNCT, Lemma=., Stopword=False\n",
            "  Entity: Barack Obama, Label: PERSON\n",
            "  Entity: Hawaii, Label: GPE\n",
            "Sentence: He was elected president in 2008.\n",
            "  He: POS=PRON, Lemma=he, Stopword=True\n",
            "  was: POS=AUX, Lemma=be, Stopword=True\n",
            "  elected: POS=VERB, Lemma=elect, Stopword=False\n",
            "  president: POS=NOUN, Lemma=president, Stopword=False\n",
            "  in: POS=ADP, Lemma=in, Stopword=True\n",
            "  2008: POS=NUM, Lemma=2008, Stopword=False\n",
            "  .: POS=PUNCT, Lemma=., Stopword=False\n",
            "  Entity: 2008, Label: DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "This program combines several spaCy features:\n",
        "\n",
        "Sentence segmentation: Splits the text into individual sentences.\n",
        "\n",
        "Token-based analysis: Prints POS, lemma, and stop word status for each word.\n",
        "\n",
        "NER per sentence: Shows entities found in each sentence.\n",
        "\n",
        "This type of analysis is foundational for many advanced NLP applications as described in the reference text"
      ],
      "metadata": {
        "id": "82nfwmICbWfO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-MyXk38baLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}